{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCkrlWMCg/CKupeG+Ep9kM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidMihalcea880/GDP2023/blob/Maisy/array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4UNSqqCtZu",
        "outputId": "caa174d5-deb1-4cdc-db13-eadd80fd1caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1139, done.\u001b[K\n",
            "remote: Total 1139 (delta 0), reused 0 (delta 0), pack-reused 1139\u001b[K\n",
            "Receiving objects: 100% (1139/1139), 70.32 MiB | 19.28 MiB/s, done.\n",
            "Resolving deltas: 100% (512/512), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/WongKinYiu/yolov7.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov7"
      ],
      "metadata": {
        "id": "9ESYrZhrC1pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -L https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt -o yolov7-w6-pose.pt"
      ],
      "metadata": {
        "id": "LBcvuX49C2gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch import nn, optim\n",
        "from utils.datasets import letterbox\n",
        "from utils.general import non_max_suppression_kpt\n",
        "from utils.plots import output_to_keypoint, plot_skeleton_kpts,plot_one_box\n",
        "from utils.torch_utils import select_device\n",
        "from models.experimental import attempt_load\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "UfZKQPyHC4dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n",
        "    # Put in inference mode\n",
        "    model.float().eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()"
      ],
      "metadata": {
        "id": "ckk2O-GxC6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def video_output(frame):\n",
        "    image = frame\n",
        "    # Apply transforms\n",
        "    image = transforms.ToTensor()(image) \n",
        "    image = image.type(torch.cuda.FloatTensor)\n",
        "    # Turn image into batch\n",
        "    image = image.unsqueeze(0) \n",
        "    output, _ = model(image)\n",
        "    output = non_max_suppression_kpt(output, \n",
        "                                     0.4, # Confidence Threshold\n",
        "                                     0.7, # IoU Threshold\n",
        "                                     nc=model.yaml['nc'], # Number of Classes\n",
        "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
        "                                     kpt_label=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = output_to_keypoint(output)\n",
        "    nimg = image[0].permute(1, 2, 0) * 255\n",
        "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
        "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
        "    for idx in range(output.shape[0]):\n",
        "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
        "    \n",
        "    return  output,nimg\n",
        "def listout(vid):\n",
        "  cap = cv2.VideoCapture(vid)\n",
        "  test=[]\n",
        "  # Get the total number of frames in the video\n",
        "  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  # Round total_frames down to nearest multiple of 30\n",
        "  rounded_frames = total_frames // 30 * 30\n",
        "  # Generate equally spaced indices for the frames\n",
        "  frame_indices = np.linspace(0, total_frames-1, rounded_frames, dtype=np.int32)\n",
        "  for i, idx in enumerate(frame_indices):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # Set the frame index\n",
        "      ret, frame = cap.read()  # Read the frame\n",
        "      if ret:  # If the frame is valid\n",
        "        frame = letterbox(frame, 640, stride=64, auto=True)[0]\n",
        "        output, nimg = video_output(frame)\n",
        "        output=output[:,2:]\n",
        "        test.append(output)\n",
        "  return test"
      ],
      "metadata": {
        "id": "0L_urA34C9YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/violence-detection-dataset.zip"
      ],
      "metadata": {
        "id": "EE0WfErqC_Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = listout('/content/yolov7/violence-detection-dataset/violent/1.mp4')"
      ],
      "metadata": {
        "id": "217feBnNDDwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape=max(np.shape(x) for x in test)"
      ],
      "metadata": {
        "id": "X7tXCjACDFtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.empty((120,2,56))\n",
        "y=np.empty([])"
      ],
      "metadata": {
        "id": "aRkUE5RPDHMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each item in the list and extract the arrays\n",
        "for i, item in enumerate(test):\n",
        "    for j in range(shape[0]):\n",
        "        x[i, j] = item[j]\n",
        "y = [1]*shape[0]"
      ],
      "metadata": {
        "id": "G93ehucKDJcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It works for 1 video. But not all video has frame count 120 and 2 people. So maybe using array isn't a good choice. but I don't know how to operate a list or other python data type."
      ],
      "metadata": {
        "id": "Z7BzM8qDD4Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp4_dir = '/content/yolov7/violence-detection-dataset/violent'\n",
        "\n",
        "# Get a list of all .avi files in the directory\n",
        "mp4_files = [f for f in os.listdir(avi_dir) if f.endswith('.mp4')]\n",
        "\n",
        "# Loop through the list of file names and pass each name to the listout function\n",
        "for mp4_file in mp4_files:\n",
        "    video_name =  os.path.join(mp4_dir, mp4_file)\n",
        "    kp=listout(video_name)\n",
        "    shape=max(np.shape(x) for x in kp)\n",
        "    # Loop over each item in the list and extract the first and second arrays\n",
        "for i, item in enumerate(test):\n",
        "    x[i, 0] = item[0]\n",
        "    x[i, 1] = item[1]\n",
        "y = [1]*shape[0]"
      ],
      "metadata": {
        "id": "AuHQ3QWiDLEm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}