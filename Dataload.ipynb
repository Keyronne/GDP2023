{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGzueFzU1lEAVtZjBrjcK/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "yx5zJVatzz6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n",
        "    # Put in inference mode\n",
        "    model.float().eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()"
      ],
      "metadata": {
        "id": "JXjHCLa7lS1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def video_output(frame):\n",
        "    image = frame\n",
        "    # Apply transforms\n",
        "    image = transforms.ToTensor()(image) \n",
        "    image = image.type(torch.cuda.FloatTensor)\n",
        "    # Turn image into batch\n",
        "    image = image.unsqueeze(0) \n",
        "    output, _ = model(image)\n",
        "    output = non_max_suppression_kpt(output, \n",
        "                                     0.25, # Confidence Threshold\n",
        "                                     0.65, # IoU Threshold\n",
        "                                     nc=model.yaml['nc'], # Number of Classes\n",
        "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
        "                                     kpt_label=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = output_to_keypoint(output)\n",
        "    nimg = image[0].permute(1, 2, 0) * 255\n",
        "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
        "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
        "    for idx in range(output.shape[0]):\n",
        "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
        "    \n",
        "    return  output,nimg"
      ],
      "metadata": {
        "id": "UFvAGG3x1rIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listout(vid):\n",
        "  cap = cv2.VideoCapture(vid)\n",
        "  test=np.empty((0,56))\n",
        "  '''\n",
        "  #I tried to have a nice dataset and since all my video is around 30-40 frames. I forced every video to have 30 frame\n",
        "  # Get the total number of frames in the video\n",
        "  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  # Generate 30 equally spaced indices for the frames\n",
        "  frame_indices = np.linspace(0, total_frames-1, 30, dtype=np.int32)\n",
        "  '''\n",
        "  for i, idx in enumerate(frame_indices):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # Set the frame index\n",
        "      ret, frame = cap.read()  # Read the frame\n",
        "      if ret:  # If the frame is valid\n",
        "        frame = letterbox(frame, 640, stride=64, auto=True)[0]\n",
        "        output, __ = video_output(frame)\n",
        "        output=output[0,2:]\n",
        "        test=np.vstack((test,output))\n",
        "  return test"
      ],
      "metadata": {
        "id": "8gZCsRuf0G-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpp3OzgzFlO"
      },
      "outputs": [],
      "source": [
        "x=np.empty((0,30,56))\n",
        "y=np.array([])\n",
        "label_map = {\"unfit\": 0, \"fit\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unfit Dataset\n",
        "\n",
        "avi_dir = '/content/yolov7/GEMEP_Vids_folders/irritation'\n",
        "\n",
        "# Get a list of all .avi files in the directory\n",
        "avi_files = [f for f in os.listdir(avi_dir) if f.endswith('.avi')]\n",
        "\n",
        "# Loop through the list of file names and pass each name to the listout function\n",
        "for avi_file in avi_files:\n",
        "    video_name =  os.path.join(avi_dir, avi_file)\n",
        "    kp=listout(video_name)\n",
        "    kp=kp[np.newaxis, ...]\n",
        "    x = np.vstack((x, kp))\n",
        "    y=np.hstack((y, np.array([0])))"
      ],
      "metadata": {
        "id": "rwbJ50C7zJon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit DataSet\n",
        "\n",
        "avi_dir = '/content/yolov7/GEMEP_Vids_folders/relief'\n",
        "\n",
        "# Get a list of all .avi files in the directory\n",
        "avi_files = [f for f in os.listdir(avi_dir) if f.endswith('.avi')]\n",
        "\n",
        "# Loop through the list of file names and pass each name to the listout function\n",
        "for avi_file in avi_files:\n",
        "    video_name =  os.path.join(avi_dir, avi_file)\n",
        "    kp=listout(video_name)\n",
        "    kp=kp[np.newaxis, ...]\n",
        "    x = np.vstack((x, kp))\n",
        "    y=np.hstack((y, np.array([1])))"
      ],
      "metadata": {
        "id": "Qk5yztmQzTWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = torch.tensor(y, device='cuda').long()\n",
        "X = torch.tensor(x, device='cuda', dtype=torch.float)\n",
        "Y=nn.functional.one_hot(Y, num_classes=2).float()"
      ],
      "metadata": {
        "id": "3nEsLfWXzhv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "# Create PyTorch DataLoader objects for batch training\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "val_dataset = TensorDataset(X_val, Y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "5NMvDcdj0tKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(train_dataset, \"train_dataset.pt\")\n",
        "torch.save(val_dataset, \"val_dataset.pt\")"
      ],
      "metadata": {
        "id": "ebJq53uf0tpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}